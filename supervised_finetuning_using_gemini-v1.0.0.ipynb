{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3oNB_qC4X2Y"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4-kxwz23nzr"
   },
   "source": [
    "# Supervised Fine Tuning with Gemini 1.5 Pro for Article Summarization\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fsupervised_finetuning_using_gemini.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/supervised_finetuning_using_gemini.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/supervised_finetuning_using_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO98gUu-4eTJ"
   },
   "source": [
    "| | | |\n",
    "|-|-|-|\n",
    "|Author(s) | [Deepak Moonat](https://github.com/dmoonat) | [Safiuddin Khaja](https://github.com/Safikh)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN72vQp6DWck"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**Gemini** is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the various Gemini models, such as Gemini 1.5 Pro, Gemini 1.0 Pro and more.\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Gemini 1.5 Pro generative model using the Vertex AI Supervised Tuning feature. Supervised Tuning allows you to use your own training data to further refine the base model's capabilities towards your specific tasks.\n",
    "\n",
    "Supervised Tuning uses labeled examples to tune a model. Each example demonstrates the output you want from your text model during inference.\n",
    "\n",
    "First, ensure your training data is of high quality, well-labeled, and directly relevant to the target task. This is crucial as low-quality data can adversely affect the performance and introduce bias in the fine-tuned model.\n",
    "- Training: Experiment with different configurations to optimize the model's performance on the target task.\n",
    "- Evaluation:\n",
    "  - Metric: Choose appropriate evaluation metrics that accurately reflect the success of the fine-tuned model for your specific task\n",
    "  - Evaluation Set: Use a separate set of data to evaluate the model's performance\n",
    "\n",
    "\n",
    "Refer to public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning) for more details.\n",
    "\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "- A Google Cloud project: Provide your project ID in the `PROJECT_ID` variable.\n",
    "\n",
    "- Authenticated your Colab environment: Run the authentication code block at the beginning.\n",
    "\n",
    "- Prepared training data (Test with your own data or use the one in the notebook): Data should be formatted in JSONL with prompts and corresponding completions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77ppk4eke7G4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you will learn how to use `Vertex AI` to tune a `gemini-1.5-pro` model.\n",
    "\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI`\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Prepare and load the dataset\n",
    "- Load the `gemini-1.5-pro-002` model\n",
    "- Evaluate the model before tuning\n",
    "- Tune the model.\n",
    "  - This will automatically create a Vertex AI endpoint and deploy the model to it\n",
    "- Make a prediction using tuned model\n",
    "- Evaluate the model after tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVRaH5wqfIy3"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfQYl84Cu_xL"
   },
   "source": [
    "## Wikilingua Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxAXgV2FvBPz"
   },
   "source": [
    "The dataset includes article and summary pairs from WikiHow. It consists of  article-summary pairs in multiple languages. Refer to the following [github repository](https://github.com/esdurmus/Wikilingua) for more details.\n",
    "\n",
    "For this notebook, we have picked `english` language dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTHBNpb-BBdc"
   },
   "source": [
    "### Dataset Citation\n",
    "\n",
    "```\n",
    "@inproceedings{ladhak-wiki-2020,\n",
    "    title={WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization},\n",
    "    author={Faisal Ladhak, Esin Durmus, Claire Cardie and Kathleen McKeown},\n",
    "    booktitle={Findings of EMNLP, 2020},\n",
    "    year={2020}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llEFILYz2aye"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo2rh4cC2e1r"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l_ok3vdw2cyf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script nltk is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --user --quiet google-cloud-aiplatform rouge_score plotly jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6in46hzz3At9"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "haJlKcSY3EsE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTOjupCM3TDb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0HMlz-MD9Yt"
   },
   "source": [
    "## Step0: Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "86VNaqlgD9rK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKRPFNzWJLVY"
   },
   "source": [
    "- If you are running this notebook in a local development environment:\n",
    "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
    "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
    "\n",
    "    ```bash\n",
    "    gcloud auth application-default login\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8CI-TcqD06L"
   },
   "source": [
    "## Step1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rerpHL_eEG8D",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jsonlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m metadata_utils\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjsonlines\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For data handling.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jsonlines'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# For extracting vertex experiment details.\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.metadata import context\n",
    "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
    "import jsonlines\n",
    "\n",
    "# For data handling.\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization.\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# For evaluation metric computation.\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For fine tuning Gemini model.\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from vertexai.preview.tuning import sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBY9nK3qEJLk"
   },
   "source": [
    "## Step2: Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VpzmI1K61Tn2",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vertexai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m PROJECT_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwiklabs-gcp-02-3a521c8c3679\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# @param\u001b[39;00m\n\u001b[1;32m      2\u001b[0m LOCATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# @param\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mvertexai\u001b[49m\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mPROJECT_ID, location\u001b[38;5;241m=\u001b[39mLOCATION)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vertexai' is not defined"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-02-3a521c8c3679\"  # @param\n",
    "LOCATION = \"us-central1\"  # @param\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUEloBlsCPFr"
   },
   "source": [
    "## Step3: Create Dataset in correct format\n",
    "\n",
    "The dataset used to tune a foundation model needs to include examples that align with the task that you want the model to perform. Structure your training dataset in a text-to-text format. Each record, or row, in the dataset contains the input text (also referred to as the prompt) which is paired with its expected output from the model. Supervised tuning uses the dataset to teach the model to mimic a behavior, or task, you need by giving it hundreds of examples that illustrate that behavior.\n",
    "\n",
    "Your dataset size depends on the task, and follows the recommendation mentioned in the `Overview` section. The more examples you provide in your dataset, the better the results.\n",
    "\n",
    "### Dataset format\n",
    "\n",
    "Training data should be structured within a JSONL file located at a Google Cloud Storage (GCS) URI. Each line (or row) of the JSONL file must adhere to a specific schema: It should contain a `contents` array, with objects inside defining a `role` (either \"user\" for user input or \"model\" for model output) and `parts`, containing the input data. For example, a valid data row would look like this:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "   \"contents\":[\n",
    "      {\n",
    "         \"role\":\"user\",  # This indicate input content\n",
    "         \"parts\":[\n",
    "            {\n",
    "               \"text\":\"How are you?\"\n",
    "            }\n",
    "         ]\n",
    "      },\n",
    "      {\n",
    "         \"role\":\"model\", # This indicate target content\n",
    "         \"parts\":[ # text only\n",
    "            {\n",
    "               \"text\":\"I am good, thank you!\"\n",
    "            }\n",
    "         ]\n",
    "      }\n",
    "      #  ... repeat \"user\", \"model\" for multi turns.\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Refer to the public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare#about-datasets) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TglfI3tQr2oZ"
   },
   "source": [
    "To run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either create a new Cloud Storage bucket or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsNdYgnaITuz"
   },
   "source": [
    "### Step3 [a]: Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WveeKANmITK5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide a bucket name\n",
    "BUCKET_NAME = \"qwiklabs-gcp-02-3a521c8c3679\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBSGTEiyJfSR"
   },
   "source": [
    "Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GQSJ3LJkJhLm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YGurtXHJy_y"
   },
   "source": [
    "### Step3 [b]: Upload tuning data to Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip8rErN2r3ah"
   },
   "source": [
    "- Data used in this notebook is present in the public Google Cloud Storage(GCS) bucket.\n",
    "- It's in Gemini 1.0 finetuning dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aV_GZg_LaNmV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/\n",
      "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_test_samples.csv\n",
      "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_train_samples.jsonl\n",
      "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_val_samples.jsonl\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b-EQ4FcExIfp",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_test_samples.csv...\n",
      "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_train_samples.jsonl...\n",
      "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_val_samples.jsonl...\n",
      "/ [3 files][  1.6 MiB/  1.6 MiB]                                                \n",
      "Operation completed over 3 objects/1.6 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wmBkAUoyzdJ"
   },
   "source": [
    "#### Convert Gemini 1.0 tuning dataset to Gemini 1.5 tuning dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7bhtxKbha_wS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_jsonlines(file, instances):\n",
    "    \"\"\"\n",
    "    Saves a list of json instances to a jsonlines file.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(file, mode=\"w\") as writer:\n",
    "        writer.write_all(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-3xbpRnSyxH9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tuning_samples(file_path):\n",
    "    \"\"\"\n",
    "    Creates tuning samples from a file.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        instances = []\n",
    "        for obj in reader:\n",
    "            instance = []\n",
    "            for content in obj[\"messages\"]:\n",
    "                instance.append(\n",
    "                    {\"role\": content[\"role\"], \"parts\": [{\"text\": content[\"content\"]}]}\n",
    "                )\n",
    "            instances.append({\"contents\": instance})\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "orHUpTagyw-z",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsonlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft_train_samples.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_instances \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tuning_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_instances)\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mcreate_tuning_samples\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_tuning_samples\u001b[39m(file_path):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Creates tuning samples from a file.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mjsonlines\u001b[49m\u001b[38;5;241m.\u001b[39mopen(file_path) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m      6\u001b[0m         instances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonlines' is not defined"
     ]
    }
   ],
   "source": [
    "train_file = \"sft_train_samples.jsonl\"\n",
    "train_instances = create_tuning_samples(train_file)\n",
    "len(train_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U3m05gAM1mlp",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save the training instances to jsonl file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m save_jsonlines(train_file, \u001b[43mtrain_instances\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_instances' is not defined"
     ]
    }
   ],
   "source": [
    "# save the training instances to jsonl file\n",
    "save_jsonlines(train_file, train_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xZICW6uU1T3Z",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsonlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m val_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft_val_samples.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m val_instances \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tuning_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(val_instances)\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mcreate_tuning_samples\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_tuning_samples\u001b[39m(file_path):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Creates tuning samples from a file.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mjsonlines\u001b[49m\u001b[38;5;241m.\u001b[39mopen(file_path) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m      6\u001b[0m         instances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsonlines' is not defined"
     ]
    }
   ],
   "source": [
    "val_file = \"sft_val_samples.jsonl\"\n",
    "val_instances = create_tuning_samples(val_file)\n",
    "len(val_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n1s7adZH1CCe",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save the validation instances to jsonl file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m save_jsonlines(val_file, \u001b[43mval_instances\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_instances' is not defined"
     ]
    }
   ],
   "source": [
    "# save the validation instances to jsonl file\n",
    "save_jsonlines(val_file, val_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AVL2gfP-J5SL",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://sft_train_samples.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n",
      "Copying file://sft_val_samples.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][231.3 KiB/231.3 KiB]                                                \n",
      "Operation completed over 1 objects/231.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Copy the tuning and evaluation data to your bucket.\n",
    "!gsutil cp {train_file} {BUCKET_URI}/train/\n",
    "!gsutil cp {val_file} {BUCKET_URI}/val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV5X6_DsIXPm"
   },
   "source": [
    "### Step3 [c]: Test dataset\n",
    "\n",
    "- It contains document text(`input_text`) and corresponding reference summary(`output_text`), which will be compared with the model generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wtxPI3GPIckU",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the test dataset using pandas as it's in the csv format.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m testing_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msft_test_samples.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(testing_data_path)\n\u001b[1;32m      4\u001b[0m test_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the test dataset using pandas as it's in the csv format.\n",
    "testing_data_path = \"sft_test_samples.csv\"\n",
    "test_data = pd.read_csv(testing_data_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRBtYfN_PPaP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data.loc[0, \"input_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTt7qjSeSHRW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Article summary stats\n",
    "stats = test_data[\"output_text\"].apply(len).describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WKptd49cdjSi",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mstats\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` test records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage length is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` and max is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` characters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsidering 1 token = 4 chars\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Total `{stats['count']}` test records\")\n",
    "print(f\"Average length is `{stats['mean']}` and max is `{stats['max']}` characters\")\n",
    "print(\"\\nConsidering 1 token = 4 chars\")\n",
    "\n",
    "# Get ceil value of the tokens required.\n",
    "tokens = (stats[\"max\"] / 4).__ceil__()\n",
    "print(\n",
    "    f\"\\nSet max_token_length = stats['max']/4 = {stats['max']/4} ~ {tokens} characters\"\n",
    ")\n",
    "print(f\"\\nLet's keep output tokens upto `{tokens}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "idM1p_UNvA7w",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Maximum number of tokens that can be generated in the response by the LLM.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Experiment with this number to get optimal output.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m max_output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Maximum number of tokens that can be generated in the response by the LLM.\n",
    "# Experiment with this number to get optimal output.\n",
    "max_output_tokens = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhjmRffOOPAS"
   },
   "source": [
    "## Step4: Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhhD1VWDsLat"
   },
   "source": [
    "The following Gemini text models support supervised tuning:\n",
    "\n",
    "* `gemini-1.5-pro-002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jL-zRl5_OVZW",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenerativeModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro-002\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m generation_model \u001b[38;5;241m=\u001b[39m \u001b[43mGenerativeModel\u001b[49m(base_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GenerativeModel' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = \"gemini-1.5-pro-002\"\n",
    "generation_model = GenerativeModel(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieJe8yGlOtFD"
   },
   "source": [
    "## Step5: Test the Gemini model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8DFUzRnHMi8"
   },
   "source": [
    "### Generation config\n",
    "\n",
    "- Each call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values\n",
    "- <strong>Experiment</strong> with different parameter values to get the best values for the task\n",
    "\n",
    "Refer to the following [link](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) for understanding different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hbaeT8AcniS"
   },
   "source": [
    "**Prompt** is a natural language request submitted to a language model to receive a response back\n",
    "\n",
    "Some best practices include\n",
    "  - Clearly communicate what content or information is most important\n",
    "  - Structure the prompt:\n",
    "    - Defining the role if using one. For example, You are an experienced UX designer at a top tech company\n",
    "    - Include context and input data\n",
    "    - Provide the instructions to the model\n",
    "    - Add example(s) if you are using them\n",
    "\n",
    "Refer to the following [link](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) for prompt design strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZUcvQr0rAWA"
   },
   "source": [
    "Wikilingua data contains the following task prompt at the end of the article, `Provide a summary of the article in two or three sentences:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iu6OuIhFOv4C",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_doc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mtest_doc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig(\n\u001b[1;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_output_tokens,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_doc = test_data.loc[0, \"input_text\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{test_doc}\n",
    "\"\"\"\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=max_output_tokens,\n",
    ")\n",
    "\n",
    "response = generation_model.generate_content(\n",
    "    contents=prompt, generation_config=generation_config\n",
    ").text\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8YvlMfmIQqK8",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ground truth\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Ground truth\n",
    "test_data.loc[0, \"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGPUKZlcP69-"
   },
   "source": [
    "## Step6: Evaluation before model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yayTQdd9oE5"
   },
   "source": [
    "- Evaluate the Gemini model on the test dataset before tuning it on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "610J64SpQ5TE",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the pandas dataframe to records (list of dictionaries).\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Check number of records.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(corpus)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the pandas dataframe to records (list of dictionaries).\n",
    "corpus = test_data.to_dict(orient=\"records\")\n",
    "# Check number of records.\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkKldH90MY4v"
   },
   "source": [
    "### Evaluation metric\n",
    "\n",
    "The type of metrics used for evaluation depends on the task that you are evaluating. The following table shows the supported tasks and the metrics used to evaluate each task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6oLtUEWMHVu"
   },
   "source": [
    "| Task             | Metric(s)                     |\n",
    "|-----------------|---------------------------------|\n",
    "| Classification   | Micro-F1, Macro-F1, Per class F1 |\n",
    "| Summarization    | ROUGE-L                         |\n",
    "| Question Answering | Exact Match                     |\n",
    "| Text Generation  | BLEU, ROUGE-L                   |\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "Refer to this [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluate-models) for metric based evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKNk3zG4CNSS"
   },
   "source": [
    "- **Recall-Oriented Understudy for Gisting Evaluation (ROUGE)**: A metric used to evaluate the quality of automatic summaries of text. It works by comparing a generated summary to a set of reference summaries created by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XP9VOaTd3z8"
   },
   "source": [
    "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
    "\n",
    "- `rouge-1`, which measures unigram overlap\n",
    "- `rouge-2`, which measures bigram overlap\n",
    "- `rouge-l`, which measures the longest common subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2CrcvvzFfBL"
   },
   "source": [
    "#### *Recall vs. Precision*\n",
    "\n",
    "**Recall**, meaning it prioritizes how much of the information in the reference summaries is captured in the generated summary.\n",
    "\n",
    "**Precision**, which measures how much of the generated summary is relevant to the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU6K4YdaGGyp"
   },
   "source": [
    "<strong>Alternate Evaluation method</strong>: Check out the [AutoSxS](https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval) evaluation for automatic evaluation of the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "p3YZAOZcQWtW",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rouge_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create rouge_scorer object for evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mrouge_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mRougeScorer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m\"\u001b[39m], use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rouge_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create rouge_scorer object for evaluation\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N8Y06N_b_EP5",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenerativeModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_evaluation\u001b[39m(model: \u001b[43mGenerativeModel\u001b[49m, corpus: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs evaluation for the given model and data.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m      A pandas DataFrame containing the evaluation results.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     records \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GenerativeModel' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_evaluation(model: GenerativeModel, corpus: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Runs evaluation for the given model and data.\n",
    "\n",
    "    Args:\n",
    "      model: The generation model.\n",
    "      corpus: The test data.\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame containing the evaluation results.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    max_retries = 5  # Maximum number of retries\n",
    "    initial_delay = 0.5  # Initial delay in seconds\n",
    "    \n",
    "    for item in tqdm(corpus):\n",
    "        document = item.get(\"input_text\")\n",
    "        summary = item.get(\"output_text\")\n",
    "        retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        while retries <= max_retries:\n",
    "\n",
    "            # Catch any exception that occur during model evaluation.\n",
    "            try:\n",
    "                response = model.generate_content(\n",
    "                    document,\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings={\n",
    "                        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Check if response is generated by the model, if response is empty then continue to next item.\n",
    "                if not (\n",
    "                    response\n",
    "                    and response.candidates\n",
    "                    and response.candidates[0].content.parts\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"\\nModel has blocked the response for the document.\\n Response: {response}\\n Document: {document}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Calculates the ROUGE score for a given reference and generated summary.\n",
    "                scores = scorer.score(target=summary, prediction=response.text)\n",
    "\n",
    "                # Append the results to the records list\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"document\": document,\n",
    "                        \"summary\": summary,\n",
    "                        \"generated_summary\": response.text,\n",
    "                        \"scores\": scores,\n",
    "                        \"rouge1_precision\": scores.get(\"rouge1\").precision,\n",
    "                        \"rouge1_recall\": scores.get(\"rouge1\").recall,\n",
    "                        \"rouge1_fmeasure\": scores.get(\"rouge1\").fmeasure,\n",
    "                        \"rouge2_precision\": scores.get(\"rouge2\").precision,\n",
    "                        \"rouge2_recall\": scores.get(\"rouge2\").recall,\n",
    "                        \"rouge2_fmeasure\": scores.get(\"rouge2\").fmeasure,\n",
    "                        \"rougeL_precision\": scores.get(\"rougeL\").precision,\n",
    "                        \"rougeL_recall\": scores.get(\"rougeL\").recall,\n",
    "                        \"rougeL_fmeasure\": scores.get(\"rougeL\").fmeasure,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                break\n",
    "                \n",
    "            except AttributeError as attr_err:\n",
    "                print(\"Attribute Error:\", attr_err)\n",
    "                continue\n",
    "            except Exception as err:\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "                delay *= 2  # Exponential backoff\n",
    "                continue\n",
    "                \n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4afTSo5cpM73",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Batch of test data.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m corpus_batch \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus\u001b[49m[:\u001b[38;5;241m100\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch of test data.\n",
    "corpus_batch = corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM10zigp7kTZ"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ It will take ~10 mins for the evaluation run on the provided batch. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aO8JhIg1pYkE",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run evaluation using loaded model and test data corpus\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m(generation_model, corpus_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "# Run evaluation using loaded model and test data corpus\n",
    "evaluation_df = run_evaluation(generation_model, corpus_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "H4VUFeb9tRBP",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "18m75w6m0R10",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluation_df_stats \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_df\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_df_stats = evaluation_df.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dYokPWqdUIMv",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Statistics of the evaluation dataframe.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluation_df_stats\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_stats' is not defined"
     ]
    }
   ],
   "source": [
    "# Statistics of the evaluation dataframe.\n",
    "evaluation_df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4fdVjY_JWcmq",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean rougeL_precision is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevaluation_df_stats\u001b[49m\u001b[38;5;241m.\u001b[39mrougeL_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_stats' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Mean rougeL_precision is\", evaluation_df_stats.rougeL_precision[\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgMb3E0YEqL2"
   },
   "source": [
    "## Step7: Fine-tune the Model\n",
    "\n",
    " - `source_model`: Specifies the base Gemini model version you want to fine-tune.\n",
    " - `train_dataset`: Path to your training data in JSONL format.\n",
    "\n",
    "  *Optional parameters*\n",
    " - `validation_dataset`: If provided, this data is used to evaluate the model during tuning.\n",
    " - `tuned_model_display_name`: Display name for the tuned model.\n",
    " - `epochs`: The number of training epochs to run.\n",
    " - `learning_rate_multiplier`: A value to scale the learning rate during training.\n",
    " - `adapter_size` : Gemini 1.5 Pro supports Adapter length [1, 4], default value is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e81137766c6"
   },
   "source": [
    "**Note: The default hyperparameter settings are optimized for optimal performance based on rigorous testing and are recommended for initial use. Users may customize these parameters to address specific performance requirements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vQM2vDBZ27b_",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tuned_model_display_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DISPLAY NAME FOR TUNED MODEL]\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# @param {type:\"string\"}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tune a model using `train` method.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sft_tuning_job \u001b[38;5;241m=\u001b[39m \u001b[43msft\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      5\u001b[0m     source_model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train/sft_train_samples.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Optional:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     validation_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val/sft_val_samples.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     tuned_model_display_name\u001b[38;5;241m=\u001b[39mtuned_model_display_name,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_model_display_name = \"[DISPLAY NAME FOR TUNED MODEL]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Tune a model using `train` method.\n",
    "sft_tuning_job = sft.train(\n",
    "    source_model=base_model,\n",
    "    train_dataset=f\"{BUCKET_URI}/train/sft_train_samples.jsonl\",\n",
    "    # Optional:\n",
    "    validation_dataset=f\"{BUCKET_URI}/val/sft_val_samples.jsonl\",\n",
    "    tuned_model_display_name=tuned_model_display_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yLlAgVjCNqXg",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the tuning job info.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msft_tuning_job\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the tuning job info.\n",
    "sft_tuning_job.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KLDCWHIJ6sxo",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the resource name of the tuning job\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sft_tuning_job_name \u001b[38;5;241m=\u001b[39m \u001b[43msft_tuning_job\u001b[49m\u001b[38;5;241m.\u001b[39mresource_name\n\u001b[1;32m      3\u001b[0m sft_tuning_job_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the resource name of the tuning job\n",
    "sft_tuning_job_name = sft_tuning_job.resource_name\n",
    "sft_tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22QZ035C8GJ3"
   },
   "source": [
    "**Note: Tuning time depends on several factors, such as training data size, number of epochs, learning rate multiplier, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN1KX-_WyKeu"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ It will take ~30 mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2ma3P6tZ6suI",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wait for job completion\n",
    "while not sft_tuning_job.refresh().has_ended:\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "O8MdqMvJHOA7",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tuned model name\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tuned_model_name \u001b[38;5;241m=\u001b[39m \u001b[43msft_tuning_job\u001b[49m\u001b[38;5;241m.\u001b[39mtuned_model_name\n\u001b[1;32m      3\u001b[0m tuned_model_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "# tuned model name\n",
    "tuned_model_name = sft_tuning_job.tuned_model_name\n",
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "e1O1xCBS6spi",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tuned model endpoint name\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tuned_model_endpoint_name \u001b[38;5;241m=\u001b[39m \u001b[43msft_tuning_job\u001b[49m\u001b[38;5;241m.\u001b[39mtuned_model_endpoint_name\n\u001b[1;32m      3\u001b[0m tuned_model_endpoint_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "# tuned model endpoint name\n",
    "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
    "tuned_model_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DzlWWKpbGcu"
   },
   "source": [
    "### Step7 [a]: Tuning and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psRbCfzwWz_g"
   },
   "source": [
    "#### Model tuning metrics\n",
    "\n",
    "- `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
    "- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
    "- `/train_num_predictions`: Number of predicted tokens at a training step\n",
    "\n",
    "#### Model evaluation metrics:\n",
    "\n",
    "- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n",
    "- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n",
    "- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\n",
    "\n",
    "The metrics visualizations are available after the model tuning job completes. If you don't specify a validation dataset when you create the tuning job, only the visualizations for the tuning metrics are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "kopo80uFbHCl",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_tuning_job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get resource name from tuning job.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[43msft_tuning_job\u001b[49m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mresource_name\n\u001b[1;32m      3\u001b[0m experiment_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sft_tuning_job' is not defined"
     ]
    }
   ],
   "source": [
    "# Get resource name from tuning job.\n",
    "experiment_name = sft_tuning_job.experiment.resource_name\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5J1LP3nCbNlg",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aiplatform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Locate Vertex AI Experiment and Vertex AI Experiment Run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241m.\u001b[39mExperiment(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name)\n\u001b[1;32m      3\u001b[0m filter_str \u001b[38;5;241m=\u001b[39m metadata_utils\u001b[38;5;241m.\u001b[39m_make_filter_string(\n\u001b[1;32m      4\u001b[0m     schema_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem.ExperimentRun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     parent_contexts\u001b[38;5;241m=\u001b[39m[experiment\u001b[38;5;241m.\u001b[39mresource_name],\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m experiment_run \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mContext\u001b[38;5;241m.\u001b[39mlist(filter_str)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aiplatform' is not defined"
     ]
    }
   ],
   "source": [
    "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
    "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
    "filter_str = metadata_utils._make_filter_string(\n",
    "    schema_title=\"system.ExperimentRun\",\n",
    "    parent_contexts=[experiment.resource_name],\n",
    ")\n",
    "experiment_run = context.Context.list(filter_str)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "htBrcQY1bPyh",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read data from Tensorboard\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tensorboard_run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39mget_backing_tensorboard_resource()\u001b[38;5;241m.\u001b[39mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_run\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(experiment\u001b[38;5;241m.\u001b[39mname,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tensorboard_run \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mTensorboardRun(tensorboard_run_name)\n\u001b[1;32m      4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m tensorboard_run\u001b[38;5;241m.\u001b[39mread_time_series_data()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Read data from Tensorboard\n",
    "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
    "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
    "metrics = tensorboard_run.read_time_series_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "uRZ-UZXcbYj5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics(metric: str = \"/train_total_loss\"):\n",
    "    \"\"\"\n",
    "    Get metrics from Tensorboard.\n",
    "\n",
    "    Args:\n",
    "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
    "    Returns:\n",
    "      steps: list of steps.\n",
    "      steps_loss: list of loss values.\n",
    "    \"\"\"\n",
    "    loss_values = metrics[metric].values\n",
    "    steps_loss = []\n",
    "    steps = []\n",
    "    for loss in loss_values:\n",
    "        steps_loss.append(loss.scalar.value)\n",
    "        steps.append(loss.step)\n",
    "    return steps, steps_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ImR4doLZblaH",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get Train and Eval Loss\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/train_total_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m get_metrics(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/eval_total_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 11\u001b[0m, in \u001b[0;36mget_metrics\u001b[0;34m(metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_metrics\u001b[39m(metric: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train_total_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Get metrics from Tensorboard.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m      steps_loss: list of loss values.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m[metric]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     12\u001b[0m     steps_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m     steps \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Get Train and Eval Loss\n",
    "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
    "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuN-m1Ikbn15"
   },
   "source": [
    "### Step7 [b]: Plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1KWWkVR5jQkA",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_subplots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the train and eval loss metrics using Plotly python library\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mmake_subplots\u001b[49m(\n\u001b[1;32m      4\u001b[0m     rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shared_xaxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subplot_titles\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Add traces\u001b[39;00m\n\u001b[1;32m      8\u001b[0m fig\u001b[38;5;241m.\u001b[39madd_trace(\n\u001b[1;32m      9\u001b[0m     go\u001b[38;5;241m.\u001b[39mScatter(x\u001b[38;5;241m=\u001b[39mtrain_loss[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mtrain_loss[\u001b[38;5;241m1\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m     col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_subplots' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the train and eval loss metrics using Plotly python library\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Steps\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Loss\")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY-eiVk0FI-M"
   },
   "source": [
    "## Step8: Load the Tuned Model\n",
    "\n",
    " - Load the fine-tuned model using `GenerativeModel` class with the tuning job model endpoint name.\n",
    "\n",
    " - Test the tuned model with the following prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GiJ831VMDQNy",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "65SYYpaNT4QR",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenerativeModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     tuned_genai_model \u001b[38;5;241m=\u001b[39m \u001b[43mGenerativeModel\u001b[49m(tuned_model_endpoint_name)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Test with the loaded model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***Testing***\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GenerativeModel' is not defined"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\n",
    "    # Test with the loaded model.\n",
    "    print(\"***Testing***\")\n",
    "    print(\n",
    "        tuned_genai_model.generate_content(\n",
    "            contents=prompt, generation_config=generation_config\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"State:\", sft_tuning_job.state)\n",
    "    print(\"Error:\", sft_tuning_job.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef7acd61d12d"
   },
   "source": [
    "```\n",
    "candidates {\n",
    "  content {\n",
    "    role: \"model\"\n",
    "    parts {\n",
    "      text: \"Squeeze a line of lotion onto the top of each forearm. Place your forearms behind your back. Rub your forearms up and down your back.\\n\\n\"\n",
    "    }\n",
    "  }\n",
    "  finish_reason: STOP\n",
    "  avg_logprobs: -0.39081838726997375\n",
    "}\n",
    "usage_metadata {\n",
    "  prompt_token_count: 261\n",
    "  candidates_token_count: 32\n",
    "  total_token_count: 293\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d54ce2b88af3"
   },
   "source": [
    "- We can clearly see the difference between summary generated pre and post tuning, as tuned summary is more inline with the ground truth format (**Note**: Pre and Post outputs, might vary based on the set parameters.)\n",
    "\n",
    "  - *Pre*: `This article describes a method for applying lotion to your own back using your forearms. The technique involves squeezing lotion in a line along your forearms, bending your elbows, and rubbing your arms against your back in a windshield wiper motion. This method may not be suitable for individuals with shoulder pain or limited flexibility.`\n",
    "  - *Post*: `Squeeze a line of lotion onto the top of each forearm. Place your forearms behind your back. Rub your forearms up and down your back`\n",
    "  - *Ground Truth*:` Squeeze a line of lotion onto the tops of both forearms and the backs of your hands. Place your arms behind your back. Move your arms in a windshield wiper motion.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYsIpFakU4CC"
   },
   "source": [
    "## Step9: Evaluation post model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwlCcKPZ62Of"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ It will take ~5 mins for the evaluation on the provided batch. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KrBk1amTU3r2",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluation_df_post_tuning \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m(tuned_genai_model, corpus_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "# run evaluation\n",
    "evaluation_df_post_tuning = run_evaluation(tuned_genai_model, corpus_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ONnlEkSex-iO",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_post_tuning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation_df_post_tuning\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_post_tuning' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_df_post_tuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xDJrlD8O0B4d",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_post_tuning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluation_df_post_tuning_stats \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_df_post_tuning\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_post_tuning' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_df_post_tuning_stats = evaluation_df_post_tuning.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "c24-mE12y4Nm",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_post_tuning_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Statistics of the evaluation dataframe post model tuning.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluation_df_post_tuning_stats\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_post_tuning_stats' is not defined"
     ]
    }
   ],
   "source": [
    "# Statistics of the evaluation dataframe post model tuning.\n",
    "evaluation_df_post_tuning_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9VU-8Ql2bqlo",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_post_tuning_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean rougeL_precision is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevaluation_df_post_tuning_stats\u001b[49m\u001b[38;5;241m.\u001b[39mrougeL_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_post_tuning_stats' is not defined"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Mean rougeL_precision is\", evaluation_df_post_tuning_stats.rougeL_precision[\"mean\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q8hN7SE08-X"
   },
   "source": [
    "#### Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "j0ctGzdnznYO",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_df_post_tuning_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\n\u001b[1;32m      2\u001b[0m     (\n\u001b[1;32m      3\u001b[0m         (\n\u001b[0;32m----> 4\u001b[0m             \u001b[43mevaluation_df_post_tuning_stats\u001b[49m\u001b[38;5;241m.\u001b[39mrougeL_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m-\u001b[39m evaluation_df_stats\u001b[38;5;241m.\u001b[39mrougeL_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m/\u001b[39m evaluation_df_stats\u001b[38;5;241m.\u001b[39mrougeL_precision[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel tuning has improved the rougeL_precision by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimprovement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (result might differ based on each tuning iteration)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_df_post_tuning_stats' is not defined"
     ]
    }
   ],
   "source": [
    "improvement = round(\n",
    "    (\n",
    "        (\n",
    "            evaluation_df_post_tuning_stats.rougeL_precision[\"mean\"]\n",
    "            - evaluation_df_stats.rougeL_precision[\"mean\"]\n",
    "        )\n",
    "        / evaluation_df_stats.rougeL_precision[\"mean\"]\n",
    "    )\n",
    "    * 100,\n",
    "    2,\n",
    ")\n",
    "print(\n",
    "    f\"Model tuning has improved the rougeL_precision by {improvement}% (result might differ based on each tuning iteration)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQkpAMnpw-jH"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esra6YPgxBiV"
   },
   "source": [
    "Performance could be further improved:\n",
    "- By adding more training samples. In general, improve your training data quality and/or quantity towards getting a more diverse and comprehensive dataset for your task\n",
    "- By tuning the hyperparameters, such as epochs and learning rate multiplier\n",
    "  - To find the optimal number of epochs for your dataset, we recommend experimenting with different values. While increasing epochs can lead to better performance, it's important to be mindful of overfitting, especially with smaller datasets. If you see signs of overfitting, reducing the number of epochs can help mitigate the issue\n",
    "- You may try different prompt structures/formats and opt for the one with better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e6fd3649040"
   },
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5528064b2cdf"
   },
   "source": [
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "Refer to this [instructions](https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-custom/cleanup#delete_resources) to delete the resources from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4dd0f5d2a21"
   },
   "outputs": [],
   "source": [
    "# Delete Experiment.\n",
    "delete_experiments = True\n",
    "if delete_experiments:\n",
    "    experiments_list = aiplatform.Experiment.list()\n",
    "    for experiment in experiments_list:\n",
    "        if experiment.resource_name == experiment_name:\n",
    "            print(experiment.resource_name)\n",
    "            experiment.delete()\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Endpoint.\n",
    "delete_endpoint = True\n",
    "# If force is set to True, all deployed models on this\n",
    "# Endpoint will be first undeployed.\n",
    "if delete_endpoint:\n",
    "    for endpoint in aiplatform.Endpoint.list():\n",
    "        if endpoint.resource_name == tuned_model_endpoint_name:\n",
    "            print(endpoint.resource_name)\n",
    "            endpoint.delete(force=True)\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Cloud Storage Bucket.\n",
    "delete_bucket = True\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "supervised_finetuning_using_gemini.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
